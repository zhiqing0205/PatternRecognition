# 5. 线性判别函数

## 问题：线性判别函数的均方误差MSE和SVM的最大间隔有何异同？

### 相同点：
两者都是用来寻找最优线性判别函数（即决策超平面）的准则。它们的目标都是确定一个权向量 **a**，从而定义一个分类边界。

### 不同点：

1. **优化目标不同**：
    - **MSE (最小平方误差)**：目标是让所有样本的函数输出 $\mathbf{a}^T\mathbf{y}$ 与预设的目标值 $b$（如+1/-1）之间的**平方误差和最小**。它是一个回归拟合问题，关心所有点到目标值的"代数"距离。
    - **SVM (最大间隔)**：目标是找到一个超平面，使其与两类中**最近的样本点**（即支持向量）的**几何距离最大化**。它只关心边界上的点，追求最大化的"安全区域"。
2. **关注的样本不同**：
    - **MSE**：其解受到**所有**训练样本的影响，包括那些远离边界的点。
    - **SVM**：其解**仅由支持向量决定**，其他远离边界的样本对最终的超平面没有影响，因此对异常值更鲁棒。
3. **结果保证不同**：
    - **MSE**：即使数据线性可分，MSE找到的解也**不保证**能将样本完全分开。
    - **SVM**：如果数据线性可分，SVM**保证**能找到一个分隔超平面，并且是间隔最大的那一个。

## 问题：SVM的最大间隔是一种归纳偏执吗？是否对所有问题都有效？为什么？

**SVM的最大间隔是一种归纳偏置**。归纳偏置是学习算法在训练数据之外进行泛化的内在假设。SVM的偏置是：**在所有能将数据分开的超平面中，那个与最近的训练样本有最大距离（即最大间隔）的超平面是最好的，因为它最有可能对新数据做出正确的分类。**

**它并非对所有问题都有效。**

原因如下：

1. **"没有免费的午餐"定理**：在机器学习中，没有任何一个算法能在所有问题上都表现最优。一个算法的有效性取决于其归纳偏置是否与特定问题的内在结构相匹配。
2. **对噪声和重叠敏感**：当两个类别的数据有大量重叠或包含很多噪声时，强制寻找一个"干净"且宽阔的间隔可能并不合适。在这种情况下，一个允许一些错误分类（通过软间隔）或者能提供概率输出（如逻辑回归）的模型可能会更实用。真实的决策边界可能本身就穿过了数据混杂的区域。
3. **计算成本**：对于超大规模的数据集，求解最大间隔所涉及的二次规划问题计算成本非常高，其他更简单的算法（如逻辑回归或朴素贝叶斯）可能在实践中更具可行性。

总之，最大间隔这个偏置在数据类别清晰、可分性较好的高维问题上非常有效，能有效防止过拟合。但当数据本身高度混杂或不符合其"最大间隔"假设时，它的效果就会打折扣。

## 问题：SVM对异常点的敏感性吗？表现在哪里？如何克服？

**经典（硬间隔）SVM对异常点非常敏感。**

### 表现在哪里：

SVM的目标是找到间隔最大的超平面。这个间隔完全由距离超平面最近的样本（即支持向量）决定。如果一个异常点恰好落在靠近决策边界的位置，它就会成为一个支持向量。为了将这个异常点也正确分类，整个决策超平面就必须向它"妥协"，这会导致：

1. **间隔急剧变小**：模型为了迁就一个异常点，牺牲了整体的"安全边界"。
2. **决策面倾斜**：超平面的方向会发生显著偏移，偏离了数据主体本应有的最优边界。

这本质上是一种过拟合，模型为了完美划分训练集（包括异常点），损害了其对新数据的泛化能力。

### 如何克服：

通过引入"**软间隔SVM (Soft Margin SVM)**"来克服这个问题。其核心思想是允许一些样本不满足严格的间隔约束（即 $z_k \cdot g(\mathbf{y}_k) \geq 1$）。

具体方法是：

1. **引入松弛变量 (Slack Variables)** $\xi_k \geq 0$：允许样本点可以进入间隔甚至被错误分类，$\xi_k$ 度量了该点违反约束的程度。
2. **设置惩罚参数 C**：在优化目标中加入惩罚项 $C \cdot \sum\xi_k$。参数 **C** 控制了"最大化间隔"和"最小化训练错误"之间的权衡。
    - **较小的 C**：对错误的惩罚小，允许更多的点违反间隔，从而忽略异常点的影响，获得更宽的间隔和更好的泛化能力。
    - **较大的 C**：对错误的惩罚大，试图将每个点都正确分类，模型行为接近硬间隔SVM，对异常点敏感。

通过调节参数C，可以控制SVM对异常点的容忍度，使其变得更加鲁棒。

## 问题：SVM的硬间隔和软间隔有何区别？软间隔的松弛变量有何作用？

### 硬间隔 (Hard Margin) SVM:

- **目标**：寻找一个能将所有训练样本**完全正确**分类，且没有任何样本点落在间隔内的超平面。
- **要求**：训练数据必须是**线性可分**的。
- **缺点**：对异常点或噪声极其敏感。一个异常点就可能导致找不到解，或者得到一个泛化能力很差的、间隔极窄的解。它是一种"零容忍"的完美主义模型。

### 软间隔 (Soft Margin) SVM:

- **目标**：在"最大化间隔"和"最小化训练错误"之间找到一个平衡。
- **要求**：不要求数据完全线性可分，更加实用和普遍。
- **优点**：通过允许一些样本被"容忍"地错分，模型对异常点和噪声更加鲁棒，泛化能力更强。

### 松弛变量 $\xi_k$ 的作用：

松弛变量是软间隔SVM的核心。它是一个为每个样本 $\mathbf{y}_k$ 引入的非负值，其作用是**量化该样本违反"硬间隔"约束的程度**。

- **$\xi_k = 0$**: 样本被正确分类，且在间隔边界上或之外（满足硬间隔要求）。
- **$0 < \xi_k \leq 1$**: 样本被正确分类，但它进入了间隔区域。
- **$\xi_k > 1$**: 样本被错误分类。

通过最小化所有松弛变量之和 $\sum\xi_k$，软间隔SVM的目标就变成了在保持间隔尽量大的同时，让违反间隔的样本数量和程度尽可能小。这使得模型能够"有策略地"忽略某些异常点，以换取更好的整体分类性能。

## 问题：SVM的核函数有何作用？会引入过拟合吗？为什么？如何克服？

### 核函数的作用：

核函数（Kernel Function）是SVM处理**非线性问题**的核心。其主要作用有两点：

1. **实现非线性映射**：它能将原始特征空间中的数据隐式地映射到一个更高维甚至无限维的特征空间中。这样做的目的是，在原始空间中线性不可分的数据，在更高维的空间中可能就变得线性可分了。
2. **避免维度灾难（"核技巧"）**：直接计算高维映射 $\phi(\mathbf{x})$ 的成本极高。核函数的精妙之处在于，它不需要显式计算这个高维向量，而是直接计算出高维空间中两个向量的内积 $K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$。这极大地降低了计算复杂度，使得在高维空间中寻找超平面变得可行。

### 会引入过拟合吗？为什么？

**是的，不恰当的核函数或参数选择会引入严重的过拟合。**

**原因**在于，核函数直接决定了决策边界的复杂度和灵活性。一个过于强大的核函数（例如，高阶多项式核，或$\gamma$ 值非常大的高斯核）可以生成一个极其扭曲和复杂的决策边界。这个边界可以完美地"记住"所有训练样本，包括其中的噪声和异常点，但在面对新的、未见过的数据时表现会很差，这就是过拟合。

### 如何克服？

克服核函数带来的过拟合主要通过**正则化**和**模型选择**：

1. **调整软间隔参数 C**：这是最重要的正则化手段。**减小参数 C** 会增大对松弛变量的容忍度，允许更多的分类错误，从而迫使模型寻找一个更简单、间隔更宽的决策边界，降低过拟合风险。
2. **选择合适的核函数及参数**：
    - 优先选择更简单的核（如线性核）。
    - 对于高斯核（RBF），关键是调整 $\gamma$ 参数。**较小的 $\gamma$ 会产生更平滑的决策边界，降低过拟合；较大的 $\gamma$ 则相反。
3. **使用交叉验证 (Cross-Validation)**：通过交叉验证在训练集上寻找最优的参数组合（如 $C$ 和 $\gamma$），而不是依赖单一训练/测试划分。这能帮助我们找到泛化能力最好的模型参数，有效避免过拟合。

## 问题：试比较显式升维和核技巧(隐式升维)的异同，并说明二者的优缺点。

### 相同点：
两者的**根本目标一致**：都是为了解决线性不可分问题。它们都试图将原始数据从一个低维空间映射到一个更高维的特征空间，期望数据在这个新空间中变得线性可分，从而可以使用线性分类器来解决。

### 不同点与优缺点：

#### 1. 显式升维 (Explicit Feature Mapping)

- **过程**：先明确定义一个映射函数 $\phi(\mathbf{x})$，然后将每一个数据点 $\mathbf{x}$ 都计算出对应的高维向量 $\mathbf{z} = \phi(\mathbf{x})$。最后，在这个新的、维度更高的数据集 $\mathbf{Z}$ 上训练一个标准的线性模型。
- **优点**：
    - **直观易懂**：过程清晰，高维特征是可见、可解释的。
    - **通用性强**：升维后的数据可以用于**任何**线性模型，不限于SVM。
- **缺点**：
    - **维度灾难与计算成本高**：如果目标维度 $\hat{d}$ 非常高，计算和存储所有高维向量 $\mathbf{z}$ 的成本将是巨大的，甚至在计算上是不可行的。

#### 2. 核技巧 (Kernel Trick / Implicit Feature Mapping)

- **过程**：它不直接计算高维向量 $\phi(\mathbf{x})$。而是巧妙地发现，像SVM这样的算法，其计算过程只依赖于样本间的**内积**（点积）。于是，它用一个**核函数 $K(\mathbf{x}_i, \mathbf{x}_j)$** 来直接计算出高维空间中的内积 $\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$，从而完全绕过了高维映射的计算。
- **优点**：
    - **计算高效**：避免了维度灾难。计算成本与高维空间的维度无关，使得在极高维甚至无限维空间中工作成为可能。
    - **功能强大**：可以轻松使用如高斯核等对应无限维空间的映射。
- **缺点**：
    - **不够直观**：过程比较抽象，像一个"黑盒"，我们无法看到高维空间中的特征究竟是什么。
    - **适用性有限**：仅适用于那些可以被"核化"（即计算过程只依赖于内积）的算法。

## 问题：请用显式和隐式升维解决一个在一维线性不可分，但在二维线性可分的例子

### 问题设定：
假设我们有一维数据点。

- **类别 $\omega_1$ (红色)**: $x = -1$ 和 $x = 1$
- **类别 $\omega_2$ (黑色)**: $x = 0$

这些点在一维直线上无法用一个点（相当于一维的"超平面"）分开。

### 1. 显式升维

**步骤1：定义映射函数 $\phi(x)$**
我们定义一个简单的二次映射，将一维 $x$ 映射到二维 $(x, x^2)$ 空间。
$$\phi(x) = [x, x^2]$$

**步骤2：计算新的高维特征向量**

- $x = -1 \rightarrow \phi(-1) = [-1, 1]$
- $x = 1 \rightarrow \phi(1) = [1, 1]$
- $x = 0 \rightarrow \phi(0) = [0, 0]$

**步骤3：在高维空间中找到线性分类器**
现在我们在二维空间 $(z_1, z_2)$ 中有了三个点：

- **$\omega_1$**: $(-1, 1)$ 和 $(1, 1)$
- **$\omega_2$**: $(0, 0)$

我们很容易找到一条水平线作为决策边界，例如 $z_2 = 0.5$。这条线能完美地将两类分开。

### 2. 核技巧 (隐式升维)

**步骤1：选择核函数**
我们选择**多项式核 (Polynomial Kernel)** $K(x_i, x_j) = (x_i \cdot x_j + c)^d$。为了模拟上面的二次映射，我们令 $c=0$, $d=2$，所以核函数为 $K(x_i, x_j) = (x_i \cdot x_j)^2$。

**步骤2：计算核矩阵**
核技巧的核心是不计算 $\phi(x)$，而是直接计算所有样本对之间的内积的核函数值。

| $K(x_i, x_j)$ | $x=-1$ | $x=1$ | $x=0$ |
| --- | --- | --- | --- |
| **$x=-1$** | 1 | 1 | 0 |
| **$x=1$** | 1 | 1 | 0 |
| **$x=0$** | 0 | 0 | 0 |

**步骤3：使用核矩阵训练模型**
像SVM这样的算法会利用这个核矩阵来求解对偶问题，找到拉格朗日乘子 $\alpha_k$。在这个过程中，它隐式地在一个由 $K$ 定义的高维空间中操作。虽然我们看不到 $\phi(x)$，但最终得到的分类器决策函数 $g(x) = \sum \alpha_k z_k K(x_k, x) + b$ 会表现出与显式升维后相同的分离能力。

**总结**：显式升维让我们"看到"了数据在高维空间中的新位置，然后画出分界线。核技巧则跳过了"看"这一步，直接通过计算内积完成了在高维空间中的"画线"操作，更加高效和强大。

## 问题：降维映射可以解决线性不可分问题吗？请举例说明

通常情况下，**降维映射不能解决线性不可分问题，反而往往会使问题变得更糟。**

其根本原因是，降维是通过**投影**将数据从高维空间映射到低维空间，这个过程会**丢失信息**。如果两类数据在高维空间中本来就混杂在一起，将它们投影到更低维的空间，只会让它们重叠得更严重，从而更难分离。

### 举例说明（一般情况）：

假设在二维空间中有两类数据：

- **$\omega_1$ (红色)**: $(2, 2)$ 和 $(3, 3)$
- **$\omega_2$ (黑色)**: $(2, -2)$ 和 $(3, -3)$

在二维空间中，它们显然是线性可分的（例如用 $y=0$ 这条线）。

现在，我们将它们**降维**，投影到一维的 x 轴上：

- **$\omega_1$** 的投影点是 $x=2$ 和 $x=3$
- **$\omega_2$** 的投影点也是 $x=2$ 和 $x=3$

降维后，两类数据完全混合在了一起，变得**线性不可分**。

### 特例（降维可以解决问题的情况）：

在极少数情况下，如果降维的**投影方向选择得极其巧妙**，它有可能使数据变得可分。此时，降维本身就扮演了一种特殊的特征提取或判别函数的角色。

### 举例说明（特殊情况）：
考虑经典的XOR问题：

- **$\omega_1$ (红色)**: $(1, 1)$ 和 $(-1, -1)$
- **$\omega_2$ (黑色)**: $(1, -1)$ 和 $(-1, 1)$

在二维空间中，它们是线性不可分的。

但是，如果我们选择一个特殊的投影方向，比如向量 $\mathbf{w} = [1, -1]$，并将所有点投影到这条线上。投影值正比于内积 $\mathbf{w}^T\mathbf{x} = x_1 - x_2$：

- **$\omega_1$** 的投影值: $1 - 1 = 0$ 和 $-1 - (-1) = 0$
- **$\omega_2$** 的投影值: $1 - (-1) = 2$ 和 $-1 - 1 = -2$

在一维的投影线上，$\omega_1$ 的点都集中在 $0$，而 $\omega_2$ 的点分布在 $-2$ 和 $2$。此时，数据变得**线性可分**了。

**结论**：虽然存在特例，但依赖降维来解决线性不可分问题是不可靠的。解决此类问题的标准和通用方法是**升维**（如使用核技巧的SVM）。

## 问题：在d维线性空间中z到超平面$\mathbf{w}^T\mathbf{x}+b=0$的距离计算能定义为一个优化问题吗？如果是，写出其目标函数并优化它

是的，点 $\mathbf{z}$ 到超平面 $\mathbf{w}^T\mathbf{x} + b = 0$ 的距离计算**可以被定义为一个经典的约束优化问题**。

其核心思想是：寻找一个在超平面上的点 $\mathbf{x}$，使其与点 $\mathbf{z}$ 的距离最短。

### 1. 目标函数与约束

这个优化问题可以形式化地表述为：

- **目标函数（最小化）**：最小化点 $\mathbf{x}$ 和点 $\mathbf{z}$ 之间的欧氏距离的平方。使用平方是为了便于求导，它与最小化原始距离是等价的。

$$\text{Minimize } f(\mathbf{x}) = \|\mathbf{x} - \mathbf{z}\|^2$$

- **约束条件**：点 $\mathbf{x}$ 必须位于超平面上。

$$\text{Subject to: } \mathbf{w}^T\mathbf{x} + b = 0$$

### 2. 优化过程（使用拉格朗日乘子法）

我们可以用拉格朗日乘子法来求解这个约束优化问题。

**步骤1：构建拉格朗日函数 $L(\mathbf{x}, \lambda)$**
$$L(\mathbf{x}, \lambda) = \|\mathbf{x} - \mathbf{z}\|^2 + \lambda(\mathbf{w}^T\mathbf{x} + b)$$
其中 $\lambda$ 是拉格朗日乘子。

**步骤2：对 $\mathbf{x}$ 求梯度并令其为零**
$$\nabla_{\mathbf{x}} L = 2(\mathbf{x} - \mathbf{z}) + \lambda\mathbf{w} = 0$$
解出 $\mathbf{x}$，得到 $\mathbf{x} = \mathbf{z} - \frac{\lambda}{2}\mathbf{w}$。
这说明，从 $\mathbf{z}$ 到其在平面上的最近点 $\mathbf{x}$ 的向量，与平面的法向量 $\mathbf{w}$ 是平行的。

**步骤3：利用约束求解 $\lambda$**
将 $\mathbf{x}$ 的表达式代入约束 $\mathbf{w}^T\mathbf{x} + b = 0$：
$$\mathbf{w}^T\left(\mathbf{z} - \frac{\lambda}{2}\mathbf{w}\right) + b = 0$$
$$\mathbf{w}^T\mathbf{z} - \frac{\lambda}{2}\mathbf{w}^T\mathbf{w} + b = 0$$
$$\mathbf{w}^T\mathbf{z} + b = \frac{\lambda}{2}\|\mathbf{w}\|^2$$
解得 $\lambda = \frac{2(\mathbf{w}^T\mathbf{z} + b)}{\|\mathbf{w}\|^2}$

**步骤4：计算距离**
我们要求的距离 $D$ 就是 $\|\mathbf{x} - \mathbf{z}\|$。根据步骤2，我们知道 $\mathbf{x} - \mathbf{z} = -\frac{\lambda}{2}\mathbf{w}$。
所以，$D = \left\|-\frac{\lambda}{2}\mathbf{w}\right\| = \left|\frac{\lambda}{2}\right| \cdot \|\mathbf{w}\|$
将 $\lambda$ 的值代入：
$$D = \left|\frac{\mathbf{w}^T\mathbf{z} + b}{\|\mathbf{w}\|^2}\right| \cdot \|\mathbf{w}\| = \frac{|\mathbf{w}^T\mathbf{z} + b|}{\|\mathbf{w}\|}$$

这个结果与教材中5.2.1节给出的距离公式完全一致。

## 支持向量机(SVM)的数学推导

支持向量机是一种基于统计学习理论的机器学习方法，其核心思想是寻找一个最优超平面，使得不同类别之间的间隔最大化。

### 1. 硬间隔SVM的数学表述

#### 1.1 问题设定

设训练样本集为 $\{(\mathbf{x}_i, y_i)\}_{i=1}^{n}$，其中：
- $\mathbf{x}_i \in \mathbb{R}^d$ 是 $d$ 维特征向量
- $y_i \in \{-1, +1\}$ 是类别标签

假设数据线性可分，我们要找到一个超平面 $\mathbf{w}^T\mathbf{x} + b = 0$ 将两类数据分开。

#### 1.2 几何间隔与函数间隔

**函数间隔**：点 $(\mathbf{x}_i, y_i)$ 到超平面的函数间隔定义为：
$$\hat{\gamma}_i = y_i(\mathbf{w}^T\mathbf{x}_i + b)$$

**几何间隔**：点 $(\mathbf{x}_i, y_i)$ 到超平面的几何间隔定义为：
$$\gamma_i = \frac{y_i(\mathbf{w}^T\mathbf{x}_i + b)}{\|\mathbf{w}\|}$$

数据集关于超平面的几何间隔为：
$$\gamma = \min_{i=1,\ldots,n} \gamma_i$$

#### 1.3 最大间隔问题的原始形式

SVM的目标是最大化几何间隔，即：

$$\begin{align}
\max_{\mathbf{w}, b} &\quad \gamma \\
\text{s.t.} &\quad \frac{y_i(\mathbf{w}^T\mathbf{x}_i + b)}{\|\mathbf{w}\|} \geq \gamma, \quad i = 1, 2, \ldots, n
\end{align}$$

为了简化计算，我们可以固定函数间隔 $\hat{\gamma} = 1$（通过重新缩放 $\mathbf{w}$ 和 $b$ 实现），原问题等价于：

$$\begin{align}
\min_{\mathbf{w}, b} &\quad \frac{1}{2}\|\mathbf{w}\|^2 \\
\text{s.t.} &\quad y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1, \quad i = 1, 2, \ldots, n
\end{align}$$

### 2. 拉格朗日对偶方法求解

#### 2.1 拉格朗日函数

引入拉格朗日乘子 $\alpha_i \geq 0$，构造拉格朗日函数：

$$L(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^{n} \alpha_i[y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1]$$

其中 $\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \ldots, \alpha_n)^T$。

#### 2.2 KKT条件

根据KKT(Karush-Kuhn-Tucker)条件，最优解 $(\mathbf{w}^*, b^*, \boldsymbol{\alpha}^*)$ 必须满足：

1. **平稳性条件**：
   $$\nabla_{\mathbf{w}} L = \mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0$$
   $$\frac{\partial L}{\partial b} = -\sum_{i=1}^{n} \alpha_i y_i = 0$$

2. **原始可行性**：
   $$y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1 \geq 0, \quad i = 1, 2, \ldots, n$$

3. **对偶可行性**：
   $$\alpha_i \geq 0, \quad i = 1, 2, \ldots, n$$

4. **互补松弛条件**：
   $$\alpha_i[y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1] = 0, \quad i = 1, 2, \ldots, n$$

#### 2.3 对偶问题推导

从平稳性条件得到：
$$\mathbf{w}^* = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i$$
$$\sum_{i=1}^{n} \alpha_i y_i = 0$$

将这些条件代入拉格朗日函数，消除 $\mathbf{w}$ 和 $b$，得到对偶问题：

$$\begin{align}
\max_{\boldsymbol{\alpha}} &\quad W(\boldsymbol{\alpha}) = \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T\mathbf{x}_j \\
\text{s.t.} &\quad \sum_{i=1}^{n} \alpha_i y_i = 0 \\
&\quad \alpha_i \geq 0, \quad i = 1, 2, \ldots, n
\end{align}$$

#### 2.4 支持向量的确定

从互补松弛条件可知：
- 如果 $\alpha_i > 0$，则 $y_i(\mathbf{w}^T\mathbf{x}_i + b) = 1$，样本 $\mathbf{x}_i$ 是**支持向量**
- 如果 $\alpha_i = 0$，则 $y_i(\mathbf{w}^T\mathbf{x}_i + b) > 1$，样本 $\mathbf{x}_i$ 不是支持向量

### 3. 软间隔SVM

#### 3.1 引入松弛变量

当数据不完全线性可分时，引入松弛变量 $\xi_i \geq 0$：

$$\begin{align}
\min_{\mathbf{w}, b, \boldsymbol{\xi}} &\quad \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{n} \xi_i \\
\text{s.t.} &\quad y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1 - \xi_i, \quad i = 1, 2, \ldots, n \\
&\quad \xi_i \geq 0, \quad i = 1, 2, \ldots, n
\end{align}$$

其中 $C > 0$ 是正则化参数，控制对错误分类的惩罚程度。

#### 3.2 软间隔SVM的对偶问题

构造拉格朗日函数：
$$L(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\mu}) = \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} \alpha_i[y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1 + \xi_i] - \sum_{i=1}^{n} \mu_i \xi_i$$

通过KKT条件求解，得到软间隔SVM的对偶问题：

$$\begin{align}
\max_{\boldsymbol{\alpha}} &\quad W(\boldsymbol{\alpha}) = \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T\mathbf{x}_j \\
\text{s.t.} &\quad \sum_{i=1}^{n} \alpha_i y_i = 0 \\
&\quad 0 \leq \alpha_i \leq C, \quad i = 1, 2, \ldots, n
\end{align}$$

与硬间隔SVM相比，唯一的区别是约束条件从 $\alpha_i \geq 0$ 变为 $0 \leq \alpha_i \leq C$。

### 4. KKT求解的具体例子

#### 4.1 简单二维例子

考虑一个简单的二维线性可分问题：

**训练数据**：
- 正类：$\mathbf{x}_1 = (3, 3)^T$，$y_1 = +1$
- 正类：$\mathbf{x}_2 = (4, 3)^T$，$y_2 = +1$  
- 负类：$\mathbf{x}_3 = (1, 1)^T$，$y_3 = -1$

#### 4.2 对偶问题求解

对偶问题为：
$$\begin{align}
\max_{\boldsymbol{\alpha}} &\quad W(\boldsymbol{\alpha}) = \alpha_1 + \alpha_2 + \alpha_3 - \frac{1}{2}[\alpha_1^2(\mathbf{x}_1^T\mathbf{x}_1) + \alpha_2^2(\mathbf{x}_2^T\mathbf{x}_2) + \alpha_3^2(\mathbf{x}_3^T\mathbf{x}_3) \\
&\quad + 2\alpha_1\alpha_2 y_1 y_2(\mathbf{x}_1^T\mathbf{x}_2) + 2\alpha_1\alpha_3 y_1 y_3(\mathbf{x}_1^T\mathbf{x}_3) + 2\alpha_2\alpha_3 y_2 y_3(\mathbf{x}_2^T\mathbf{x}_3)] \\
\text{s.t.} &\quad \alpha_1 + \alpha_2 - \alpha_3 = 0 \\
&\quad \alpha_i \geq 0, \quad i = 1, 2, 3
\end{align}$$

**计算内积**：
- $\mathbf{x}_1^T\mathbf{x}_1 = 3^2 + 3^2 = 18$
- $\mathbf{x}_2^T\mathbf{x}_2 = 4^2 + 3^2 = 25$
- $\mathbf{x}_3^T\mathbf{x}_3 = 1^2 + 1^2 = 2$
- $\mathbf{x}_1^T\mathbf{x}_2 = 3 \times 4 + 3 \times 3 = 21$
- $\mathbf{x}_1^T\mathbf{x}_3 = 3 \times 1 + 3 \times 1 = 6$
- $\mathbf{x}_2^T\mathbf{x}_3 = 4 \times 1 + 3 \times 1 = 7$

**目标函数**：
$$W(\boldsymbol{\alpha}) = \alpha_1 + \alpha_2 + \alpha_3 - \frac{1}{2}[18\alpha_1^2 + 25\alpha_2^2 + 2\alpha_3^2 + 42\alpha_1\alpha_2 - 12\alpha_1\alpha_3 - 14\alpha_2\alpha_3]$$

#### 4.3 利用约束条件简化

由约束条件 $\alpha_1 + \alpha_2 - \alpha_3 = 0$，得 $\alpha_3 = \alpha_1 + \alpha_2$。

代入目标函数并化简：
$$W(\alpha_1, \alpha_2) = 2\alpha_1 + 2\alpha_2 - \frac{1}{2}[37\alpha_1^2 + 54\alpha_2^2 + 70\alpha_1\alpha_2]$$

#### 4.4 求解最优解

对 $\alpha_1$ 和 $\alpha_2$ 求偏导并令其为零：
$$\frac{\partial W}{\partial \alpha_1} = 2 - 37\alpha_1 - 35\alpha_2 = 0$$
$$\frac{\partial W}{\partial \alpha_2} = 2 - 54\alpha_2 - 35\alpha_1 = 0$$

解得：
$$\alpha_1 = \frac{2}{37}, \quad \alpha_2 = \frac{2}{54} = \frac{1}{27}, \quad \alpha_3 = \frac{2}{37} + \frac{1}{27} = \frac{91}{999}$$

#### 4.5 计算最优参数

**权重向量**：
$$\mathbf{w}^* = \sum_{i=1}^{3} \alpha_i y_i \mathbf{x}_i = \frac{2}{37}(3, 3)^T + \frac{1}{27}(4, 3)^T - \frac{91}{999}(1, 1)^T$$

**偏置项**：利用支持向量上的约束条件 $y_i(\mathbf{w}^{*T}\mathbf{x}_i + b^*) = 1$。

#### 4.6 KKT条件验证

最优解必须满足所有KKT条件：

1. **平稳性**：$\mathbf{w}^* = \sum_{i=1}^{3} \alpha_i^* y_i \mathbf{x}_i$ ✓
2. **原始可行性**：$y_i(\mathbf{w}^{*T}\mathbf{x}_i + b^*) \geq 1$ 对所有 $i$ 成立 ✓
3. **对偶可行性**：$\alpha_i^* \geq 0$ 对所有 $i$ 成立 ✓  
4. **互补松弛**：由于所有 $\alpha_i^* > 0$，所有样本都是支持向量，满足 $y_i(\mathbf{w}^{*T}\mathbf{x}_i + b^*) = 1$ ✓

### 5. 决策函数

求解完成后，SVM的决策函数为：
$$f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{n} \alpha_i^* y_i \mathbf{x}_i^T\mathbf{x} + b^*\right)$$

只有支持向量（$\alpha_i > 0$ 的样本）对决策函数有贡献，这体现了SVM的稀疏性特点。

### 6. 核化SVM

对于非线性问题，通过核函数 $K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$ 将原始特征空间映射到高维特征空间，对偶问题变为：

$$\begin{align}
\max_{\boldsymbol{\alpha}} &\quad W(\boldsymbol{\alpha}) = \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j) \\
\text{s.t.} &\quad \sum_{i=1}^{n} \alpha_i y_i = 0 \\
&\quad 0 \leq \alpha_i \leq C, \quad i = 1, 2, \ldots, n
\end{align}$$

决策函数变为：
$$f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{n} \alpha_i^* y_i K(\mathbf{x}_i, \mathbf{x}) + b^*\right)$$

KKT条件和求解方法保持不变，只是将内积 $\mathbf{x}_i^T\mathbf{x}_j$ 替换为核函数 $K(\mathbf{x}_i, \mathbf{x}_j)$。